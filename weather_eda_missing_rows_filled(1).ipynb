{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('weather_noduplicates.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### The goal is to find and fill blanks in the dataset.\n",
    "#### We iterate over rows and compare current timestamp with the next timestamp to check if there are any gaps in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gap found \n",
      "40377 08.10.2009 09:40:00 \n",
      "40378 08.10.2009 10:10:00\n",
      "\n",
      "Timedelta 1800 sec. 2 rows missing \n",
      "\n",
      "\n",
      "Gap found \n",
      "229874 16.05.2013 08:50:00 \n",
      "229875 16.05.2013 09:10:00\n",
      "\n",
      "Timedelta 1200 sec. 1 rows missing \n",
      "\n",
      "\n",
      "Gap found \n",
      "293228 30.07.2014 08:00:00 \n",
      "293229 30.07.2014 08:20:00\n",
      "\n",
      "Timedelta 1200 sec. 1 rows missing \n",
      "\n",
      "\n",
      "Gap found \n",
      "301345 24.09.2014 17:00:00 \n",
      "301346 25.09.2014 09:00:00\n",
      "\n",
      "Timedelta 57600 sec. 95 rows missing \n",
      "\n",
      "\n",
      "Gap found \n",
      "410939 25.10.2016 10:30:00 \n",
      "410940 28.10.2016 12:50:00\n",
      "\n",
      "Timedelta 267600 sec. 445 rows missing \n",
      "\n",
      "\n",
      "\n",
      "No more rows to check. \n",
      "Last row was 420223 01.01.2017 00:00:00\n",
      "\n",
      "\n",
      "Total missing rows :  544\n"
     ]
    }
   ],
   "source": [
    "time_format = '%d.%m.%Y %H:%M:%S'\n",
    "step_timedelta = 600     # one row is 10 minutes * 60 seconds\n",
    "\n",
    "def find_missing_rows(df):\n",
    "    missing_rows = pd.DataFrame()\n",
    "    total_missing_rows = 0\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            next_row = df.iloc[idx+1]\n",
    "        except IndexError:\n",
    "            print('\\nNo more rows to check. \\nLast row was {} {}\\n\\n'.format(idx, row['Date Time']))\n",
    "\n",
    "        current_ts = datetime.strptime(row['Date Time'], time_format)\n",
    "        next_ts = datetime.strptime(next_row['Date Time'], time_format)\n",
    "\n",
    "        # compare the difference between current timestamp and the next timestamp to our step\n",
    "        delta_seconds = (next_ts - current_ts).total_seconds()\n",
    "        if delta_seconds > step_timedelta:\n",
    "            missing_rows = missing_rows.append(row)\n",
    "            print('Gap found \\n{} {} \\n{} {}\\n'.format(idx, row['Date Time'], idx+1, next_row['Date Time']))\n",
    "\n",
    "            # total missing seconds between timestamps is divided by our step \n",
    "            rows_missing = int(delta_seconds / step_timedelta) - 1\n",
    "            print('Timedelta {} sec. {} rows missing \\n\\n'.format(int(delta_seconds), rows_missing))\n",
    "            total_missing_rows += rows_missing\n",
    "\n",
    "    print('Total missing rows : ', total_missing_rows)\n",
    "    return total_missing_rows, missing_rows\n",
    "    \n",
    "total_missing_rows, missing_rows = find_missing_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_days = 6*365 + 2*366  # 2012 and 2016 are leap years\n",
    "intervals_in_day = 6 * 24   # interval is ten minutes\n",
    "total_intervals = intervals_in_day * total_days\n",
    "total_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sanity check\n",
    "total_intervals - df.shape[0] == total_missing_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "#### We've found 544 missing rows. The main dataset is from Beutenberg WS. Lets check our data against Saaleaue WS data samples to see if we can fill missing rows using their observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function selects 10 random rows from df1 and checks \n",
    "# the same datetimes are present in df2 to make sure dataframes are from the same timeframe\n",
    "\n",
    "def dataset_match_random_check(df1, df2):\n",
    "    max_n = df1.shape[0] if df1.shape[0] < df2.shape[0] else df2.shape[0]  # max index from the smaller df\n",
    "    indexes = np.random.randint(0, max_n, 10).tolist()\n",
    "    for i in indexes:\n",
    "        row = df1.iloc[i]\n",
    "        try:\n",
    "            df2.loc[df2['Date Time'] == row['Date Time']]\n",
    "        except:\n",
    "            print('Row with datetime {} not found'.format(row['Date Time']))\n",
    "    print('Check done, all ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data samples from Max-Plank Institute weather station\n",
    "# https://www.bgc-jena.mpg.de/wetter/weather_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check done, all ok\n",
      "Check done, all ok\n",
      "Check done, all ok\n",
      "Check done, all ok\n"
     ]
    }
   ],
   "source": [
    "saale_2009 = pd.read_csv('mpi_saale_2009b.csv', delimiter=',', encoding=\"ISO-8859-1\")\n",
    "saale_2013 = pd.read_csv('mpi_saale_2013a.csv', delimiter=',', encoding=\"ISO-8859-1\")\n",
    "saale_2014 = pd.read_csv('mpi_saale_2014b.csv', delimiter=',', encoding=\"ISO-8859-1\")\n",
    "saale_2016 = pd.read_csv('mpi_saale_2016b.csv', delimiter=',', encoding=\"ISO-8859-1\")\n",
    "dataset_match_random_check(df, saale_2009)\n",
    "dataset_match_random_check(df, saale_2013)\n",
    "dataset_match_random_check(df, saale_2014)\n",
    "dataset_match_random_check(df, saale_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date Time</th>\n",
       "      <th>H2OC (mmol/mol)</th>\n",
       "      <th>T (degC)</th>\n",
       "      <th>Tdew (degC)</th>\n",
       "      <th>Tpot (K)</th>\n",
       "      <th>VPact (mbar)</th>\n",
       "      <th>VPdef (mbar)</th>\n",
       "      <th>VPmax (mbar)</th>\n",
       "      <th>max. wv (m/s)</th>\n",
       "      <th>p (mbar)</th>\n",
       "      <th>rh (%)</th>\n",
       "      <th>rho (g/m**3)</th>\n",
       "      <th>sh (g/kg)</th>\n",
       "      <th>wd (deg)</th>\n",
       "      <th>wv (m/s)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40377</th>\n",
       "      <td>08.10.2009 09:40:00</td>\n",
       "      <td>16.13</td>\n",
       "      <td>17.80</td>\n",
       "      <td>13.86</td>\n",
       "      <td>292.37</td>\n",
       "      <td>15.86</td>\n",
       "      <td>4.55</td>\n",
       "      <td>20.41</td>\n",
       "      <td>5.38</td>\n",
       "      <td>983.19</td>\n",
       "      <td>77.70</td>\n",
       "      <td>1169.97</td>\n",
       "      <td>10.10</td>\n",
       "      <td>248.50</td>\n",
       "      <td>1.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229874</th>\n",
       "      <td>16.05.2013 08:50:00</td>\n",
       "      <td>15.15</td>\n",
       "      <td>16.69</td>\n",
       "      <td>12.74</td>\n",
       "      <td>292.08</td>\n",
       "      <td>14.75</td>\n",
       "      <td>4.28</td>\n",
       "      <td>19.03</td>\n",
       "      <td>3.50</td>\n",
       "      <td>973.59</td>\n",
       "      <td>77.50</td>\n",
       "      <td>1163.42</td>\n",
       "      <td>9.48</td>\n",
       "      <td>54.28</td>\n",
       "      <td>1.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293228</th>\n",
       "      <td>30.07.2014 08:00:00</td>\n",
       "      <td>21.35</td>\n",
       "      <td>19.71</td>\n",
       "      <td>18.22</td>\n",
       "      <td>294.44</td>\n",
       "      <td>20.96</td>\n",
       "      <td>2.05</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.60</td>\n",
       "      <td>981.50</td>\n",
       "      <td>91.10</td>\n",
       "      <td>1158.04</td>\n",
       "      <td>13.39</td>\n",
       "      <td>101.80</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301345</th>\n",
       "      <td>24.09.2014 17:00:00</td>\n",
       "      <td>10.18</td>\n",
       "      <td>14.04</td>\n",
       "      <td>6.99</td>\n",
       "      <td>288.49</td>\n",
       "      <td>10.02</td>\n",
       "      <td>6.03</td>\n",
       "      <td>16.05</td>\n",
       "      <td>2.20</td>\n",
       "      <td>984.50</td>\n",
       "      <td>62.44</td>\n",
       "      <td>1189.56</td>\n",
       "      <td>6.36</td>\n",
       "      <td>191.00</td>\n",
       "      <td>1.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410939</th>\n",
       "      <td>25.10.2016 10:30:00</td>\n",
       "      <td>11.04</td>\n",
       "      <td>9.48</td>\n",
       "      <td>8.33</td>\n",
       "      <td>283.07</td>\n",
       "      <td>10.98</td>\n",
       "      <td>0.89</td>\n",
       "      <td>11.87</td>\n",
       "      <td>2.24</td>\n",
       "      <td>994.67</td>\n",
       "      <td>92.50</td>\n",
       "      <td>1220.83</td>\n",
       "      <td>6.90</td>\n",
       "      <td>144.10</td>\n",
       "      <td>1.53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Date Time  H2OC (mmol/mol)  T (degC)  Tdew (degC)  Tpot (K)  \\\n",
       "40377   08.10.2009 09:40:00            16.13     17.80        13.86    292.37   \n",
       "229874  16.05.2013 08:50:00            15.15     16.69        12.74    292.08   \n",
       "293228  30.07.2014 08:00:00            21.35     19.71        18.22    294.44   \n",
       "301345  24.09.2014 17:00:00            10.18     14.04         6.99    288.49   \n",
       "410939  25.10.2016 10:30:00            11.04      9.48         8.33    283.07   \n",
       "\n",
       "        VPact (mbar)  VPdef (mbar)  VPmax (mbar)  max. wv (m/s)  p (mbar)  \\\n",
       "40377          15.86          4.55         20.41           5.38    983.19   \n",
       "229874         14.75          4.28         19.03           3.50    973.59   \n",
       "293228         20.96          2.05         23.00           0.60    981.50   \n",
       "301345         10.02          6.03         16.05           2.20    984.50   \n",
       "410939         10.98          0.89         11.87           2.24    994.67   \n",
       "\n",
       "        rh (%)  rho (g/m**3)  sh (g/kg)  wd (deg)  wv (m/s)  \n",
       "40377    77.70       1169.97      10.10    248.50      1.93  \n",
       "229874   77.50       1163.42       9.48     54.28      1.78  \n",
       "293228   91.10       1158.04      13.39    101.80      0.31  \n",
       "301345   62.44       1189.56       6.36    191.00      1.52  \n",
       "410939   92.50       1220.83       6.90    144.10      1.53  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these are target values to look for to complete the dataset df \n",
    "missing_rows  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_corresponding_rows(target_value, target_row, n_rows, df):\n",
    "    new_row = df.loc[df[target_row] == target_value] \n",
    "    idx_ = new_row.index[0]\n",
    "    sliced = df[idx_+1:idx_+n_rows+1]\n",
    "    return sliced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_value_1 = '25.10.2016 10:30:04'\n",
    "target_value_2 = '25.10.2016 10:30:00'\n",
    "n_rows_missing = 445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: offset is 4 sec\n",
    "new_data = find_corresponding_rows(target_value_1, 'Date Time', n_rows_missing, saale_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 31)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_columns(original_data, new_data):\n",
    "    original_columns = original_data.columns.tolist()\n",
    "    new_columns = new_data.columns.tolist()\n",
    "    \n",
    "    for column in new_columns:\n",
    "        if column not in original_columns:\n",
    "            new_data.drop(column, axis=1, inplace=True)\n",
    "        \n",
    "    if len(new_data.columns.tolist()) < len(original_columns):\n",
    "        for c in original_columns:\n",
    "            if c not in new_data.columns.tolist():\n",
    "                new_data = new_data.assign(c=np.nan)\n",
    "\n",
    "    assert len(new_data.columns.tolist()) == len(original_columns)\n",
    "    new_data = new_data.reindex(columns=original_columns)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 15)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adjusted_data = adjust_columns(df, new_data)\n",
    "adjusted_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_offset(pd_series, offset):\n",
    "    time_format = '%d.%m.%Y %H:%M:%S'\n",
    "    for i, value in pd_series.iteritems():\n",
    "        new_value = datetime.strptime(value, time_format) - offset\n",
    "        pd_series.loc[i] = datetime.strftime(new_value, time_format)\n",
    "    return pd_series\n",
    "    \n",
    "corrected_data = correct_offset(adjusted_data['Date Time'], timedelta(seconds=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411038"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find init position to insert new rows\n",
    "init_index = df_new.loc[df_new['Date Time'] == target_value_2].index[0]\n",
    "init_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([df_new.iloc[:init_index+1], \n",
    "                    adjusted_data, \n",
    "                    df_new.iloc[init_index+1:]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420768, 15)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.shape[0] == total_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv('weather_filled.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "No more rows to check. \n",
      "Last row was 420767 01.01.2017 00:00:00\n",
      "\n",
      "\n",
      "Total missing rows :  0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, Empty DataFrame\n",
       " Columns: []\n",
       " Index: [])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_missing_rows(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
